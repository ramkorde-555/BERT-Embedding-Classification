{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5eff31ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import time\n",
    "\n",
    "#Algorithms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "\n",
    "# Text Processing\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk import FreqDist\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "### Sentiment Analysis\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "### Dimensionality Reduction\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import random_projection\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "### Scaling\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler,normalize\n",
    "\n",
    "### Dataset Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Evalution Metric\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "#from keras import metrics\n",
    "\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d6f4269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING DATASET\n",
    "training_set = pd.read_csv(\"train.csv\")\n",
    "testing_set = pd.read_csv(\"test.csv\")\n",
    "sample_submission = pd.read_csv(\"sample_submission.csv\")\n",
    "\n",
    "# LoADING STOPWORDS\n",
    "with open('stopwords.txt') as f:\n",
    "    stop_words = f.read().splitlines()\n",
    "\n",
    "# LOADING SPACY ENGLISH LANGUAGE MODEL\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# LOADING WORD LIST FROM NLTK\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "# DECLARING LEMMATIZER OBJECT AND SENTIMENT OBJECT\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4a04218",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #DECLARE PUNCTUATION STRING\n",
    "string.punctuation = '!#\"$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c601e91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_words(df):    \n",
    "    text_list = []\n",
    "    less_frequent_word = []\n",
    "    for rec in df[\"text\"]:\n",
    "        for words in rec.split():\n",
    "            text_list.append(words)\n",
    "    freqDist = FreqDist(text_list)\n",
    "    words = list(freqDist.keys())\n",
    "    for wrd in words:\n",
    "        if freqDist[wrd] <= 3:\n",
    "            less_frequent_word.append(wrd)\n",
    "    return less_frequent_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2965f6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_drop_train = []\n",
    "col_to_drop_test = []\n",
    "less_freq_wrd_train = freq_words(training_set)\n",
    "less_freq_wrd_test = freq_words(testing_set)\n",
    "less_freq_wrd = less_freq_wrd_test + less_freq_wrd_train\n",
    "once_present_word = list(set(less_freq_wrd))\n",
    "wrds_ignore = once_present_word+stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b1ebb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    sentiment_value = 0\n",
    "    sentence = \"\"\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    for wrd in text.split():\n",
    "        if wrd not in wrds_ignore:\n",
    "            #if wrd in words:\n",
    "            if wrd.isalpha():\n",
    "                wrd = lemmatizer.lemmatize(wrd)\n",
    "                if wrd not in sentence:\n",
    "                    sentence += \" \".join(wrd.split())+\" \"\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eca03b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set[\"text\"] = training_set[\"text\"].apply(text_preprocessing)\n",
    "testing_set[\"text\"] = testing_set[\"text\"].apply(text_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9baad783",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = []\n",
    "for dataframe in [training_set[\"text\"],testing_set[\"text\"]]:\n",
    "    for sentence in dataframe:\n",
    "        for words in sentence.split():\n",
    "            dictionary.append(words)\n",
    "            \n",
    "unique_dictionary = list(set(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a6c1c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text,iD in zip(testing_set[testing_set.keyword.isnull()][\"text\"],testing_set[testing_set.keyword.isnull()][\"text\"].index):\n",
    "    doc = nlp(text)\n",
    "    wrds_pos = []\n",
    "    for ent in doc.ents: \n",
    "        if ent.label_ == \"GPE\":\n",
    "            #testing_set.loc[iD,\"keyword\"] = ent.text\n",
    "            #testing_set.loc[iD,\"location\"] = ent.text\n",
    "            wrds_pos.append(ent.text)\n",
    "        elif ent.label_ == \"NORP\":\n",
    "            #testing_set.loc[iD,\"keyword\"] = ent.text\n",
    "            wrds_pos.append(ent.text)\n",
    "        elif ent.label_ == \"LOC\":\n",
    "            #testing_set.loc[iD,\"keyword\"] = ent.text\n",
    "            wrds_pos.append(ent.text)\n",
    "        elif ent.label_ == \"ORG\":\n",
    "            #testing_set.loc[iD,\"keyword\"] = ent.text\n",
    "            wrds_pos.append(ent.text)\n",
    "        elif ent.label_ ==\"EVENT\":\n",
    "            #testing_set.loc[iD,\"keyword\"] = ent.text\n",
    "            wrds_pos.append(ent.text)\n",
    "        elif ent.label_ == \"FAC\":\n",
    "            #testing_set.loc[iD,\"keyword\"] = ent.text\n",
    "            wrds_pos.append(ent.text)\n",
    "        elif ent.label_ == \"LANGUAGE\":\n",
    "            #testing_set.loc[iD,\"keyword\"] = ent.text\n",
    "            wrds_pos.append(ent.text)\n",
    "        elif ent.label_ == \"PRODUCT\":\n",
    "            #testing_set.loc[iD,\"keyword\"] = ent.text\n",
    "            wrds_pos.append(ent.text)\n",
    "#         elif ent.label_ == 'WORK_OF_ART':\n",
    "#             testing_set.loc[iD,\"keyword\"] = ent.text\n",
    "#             wrds_pos.append(ent.text)\n",
    "        else:\n",
    "            wrds_pos.append(\"\")\n",
    "    for token in nlp(text):\n",
    "        if token.pos_ == \"NOUN\":\n",
    "            #testing_set.loc[iD,\"keyword\"] = token.text\n",
    "            wrds_pos.append(token.text)\n",
    "        elif token.pos_ == \"VERB\":\n",
    "            #testing_set.loc[iD,\"keyword\"] = token.text\n",
    "            wrds_pos.append(token.text)\n",
    "#         elif token.pos_ == \"PROPN\":\n",
    "#             testing_set.loc[iD,\"keyword\"] = token.text\n",
    "#             wrds_pos.append(token.text)\n",
    "#         elif token.pos_ == \"ADJ\":\n",
    "#             testing_set.loc[iD,\"keyword\"] = token.text\n",
    "#             wrds_pos.append(token.text)\n",
    "#         if token.pos_ == \"ADV\":\n",
    "#             testing_set.loc[iD,\"keyword\"] = token.text\n",
    "#             wrds_pos.append(token.text)\n",
    "    sent = \" \".join(wrd for wrd in wrds_pos)\n",
    "    testing_set.loc[iD,\"text\"] = sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9174428f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text,iD in zip(training_set[training_set.keyword.isnull()][\"text\"],training_set[training_set.keyword.isnull()][\"text\"].index):\n",
    "    doc = nlp(text)\n",
    "    wrds_pos = []\n",
    "    for ent in doc.ents: \n",
    "        if ent.label_ == \"GPE\":\n",
    "            #training_set.loc[iD,\"keyword\"] = ent.text\n",
    "            #training_set.loc[iD,\"location\"] = ent.text\n",
    "            wrds_pos.append(ent.text)\n",
    "        elif ent.label_ == \"NORP\":\n",
    "            #training_set.loc[iD,\"keyword\"] = ent.text\n",
    "            wrds_pos.append(ent.text)\n",
    "        elif ent.label_ == \"ORG\":\n",
    "            #training_set.loc[iD,\"keyword\"] = ent.text\n",
    "            wrds_pos.append(ent.text)\n",
    "        elif ent.label_ == \"LOC\":\n",
    "            #training_set.loc[iD,\"keyword\"] = ent.text\n",
    "            wrds_pos.append(ent.text)\n",
    "        elif ent.label_ ==\"EVENT\":\n",
    "            #training_set.loc[iD,\"keyword\"] = ent.text\n",
    "            wrds_pos.append(ent.text)\n",
    "        elif ent.label_ == \"FAC\":\n",
    "            #training_set.loc[iD,\"keyword\"] = ent.text\n",
    "            wrds_pos.append(ent.text)\n",
    "        elif ent.label_ == \"LANGUAGE\":\n",
    "            #training_set.loc[iD,\"keyword\"] = ent.text\n",
    "            wrds_pos.append(ent.text)\n",
    "        elif ent.label_ == \"PRODUCT\":\n",
    "            #training_set.loc[iD,\"keyword\"] = ent.text\n",
    "            wrds_pos.append(ent.text)\n",
    "#         elif ent.label_ == 'WORK_OF_ART':\n",
    "#             training_set.loc[iD,\"keyword\"] = ent.text\n",
    "#             wrds_pos.append(ent.text)\n",
    "#         elif ent.label_ == 'PERSON':\n",
    "#             training_set.loc[iD,\"keyword\"] = ent.text\n",
    "#             wrds_pos.append(ent.text)\n",
    "        else:\n",
    "            wrds_pos.append(\"\")\n",
    "    for token in nlp(text):\n",
    "        if token.pos_ == \"NOUN\":\n",
    "            #training_set.loc[iD,\"keyword\"] = token.text\n",
    "            wrds_pos.append(token.text)\n",
    "        elif token.pos_ == \"VERB\":\n",
    "            #training_set.loc[iD,\"keyword\"] = token.text\n",
    "            wrds_pos.append(token.text)\n",
    "#         elif token.pos_ == \"PROPN\":\n",
    "#             training_set.loc[iD,\"keyword\"] = token.text\n",
    "#             wrds_pos.append(token.text)\n",
    "#         elif token.pos_ == \"ADJ\":\n",
    "#             training_set.loc[iD,\"keyword\"] = token.text\n",
    "#             wrds_pos.append(token.text)\n",
    "#         if token.pos_ == \"ADV\":\n",
    "#             training_set.loc[iD,\"keyword\"] = token.text\n",
    "#             wrds_pos.append(token.text)\n",
    "    sent = \" \".join(wrd for wrd in wrds_pos)\n",
    "    training_set.loc[iD,\"text\"] = sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bdec775c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating label\n",
    "y = training_set[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7fd48a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vector = TfidfVectorizer(ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e85eb790",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vector_train = pd.DataFrame(tfidf_vector.fit_transform(training_set[\"text\"].apply(lambda x: np.str_(x))).toarray(),columns = tfidf_vector.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "39c60fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vector_test = pd.DataFrame(tfidf_vector.transform(testing_set[\"text\"].apply(lambda x: np.str_(x))).toarray(),columns = tfidf_vector.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d902d8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the total set: 7613\n",
      "Number of rows in the training set: 6090\n",
      "Number of rows in the test set: 1523\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(text_vector_train, y,random_state=42, test_size=0.2)\n",
    "\n",
    "print('Number of rows in the total set: {}'.format(text_vector_train.shape[0]))\n",
    "print('Number of rows in the training set: {}'.format(X_train.shape[0]))\n",
    "print('Number of rows in the test set: {}'.format(X_valid.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bbcd7bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictionary of models\n",
    "classifiers = {'BerNB': BernoulliNB(),'Logistic Regression':LogisticRegression()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f1b253c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score of BerNB : 0.7669074195666448\n",
      "Accuracy Score of Logistic Regression : 0.7715036112934996\n"
     ]
    }
   ],
   "source": [
    "#Iterating through the dataset with all model declared in the dataset.\n",
    "base_accuracy = 0\n",
    "for Name,classify in classifiers.items():\n",
    "    classify.fit(X_train,y_train)\n",
    "    y_predictng = classify.predict(X_valid)\n",
    "    print('Accuracy Score of '+str(Name) + \" : \" +str(accuracy_score(y_valid,y_predictng)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4f7b6f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "pID = sample_submission[\"id\"]\n",
    "predicted_test = classify.predict(text_vector_test)\n",
    "predicted_test_value = pd.DataFrame({ 'id': pID,\n",
    "                        'target': predicted_test })\n",
    "predicted_test_value.to_csv(\"PredictedTestScore.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4459feca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle-nlp",
   "language": "python",
   "name": "kaggle-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
